{
  "metadata": {
    "name": "Status Updater",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python.ipython\n\nimport logging\nimport random\nimport string\nimport requests\nimport re\nimport pandas as pd\nimport numpy as np\nimport time\nfrom tqdm import tqdm\n\nfrom cassandra import ConsistencyLevel\nfrom cassandra.cluster import Cluster\nfrom cassandra.query import SimpleStatement\n\nKEYSPACE \u003d \"gdelt_prod\"\ncluster \u003d Cluster([\u0027192.168.3.74\u0027])\nsession \u003d cluster.connect()\nsession.set_keyspace(KEYSPACE)\n\nresponse \u003d requests.get(\u0027http://data.gdeltproject.org/gdeltv2/masterfilelist.txt\u0027, allow_redirects\u003dTrue)\ntext \u003d response.content.decode()\n\ndf \u003d pd.DataFrame(re.split(\" |\\n\", text), columns\u003d[\"champs\"])\ndf[\"zip\"] \u003d df[\"champs\"].apply(lambda x: 1 if \".zip\" in x else \"\")\ndf \u003d df[df[\"zip\"] \u003d\u003d 1]\ndf[\"date\"] \u003d df[\"champs\"].apply(lambda x: re.findall(\"2\\d+\", x)[0])\ndf[\"date\"] \u003d df[\"date\"].astype(int)\n\ndf \u003d df.groupby(\u0027date\u0027).sum()\ndf \u003d df[df[\"zip\"]\u003d\u003d3]\ndf \u003d df.drop([\u0027champs\u0027,\u0027zip\u0027],axis\u003d1)\n\ndf[\u0027nb_articles_events\u0027] \u003d \"NA\"\ndf[\u0027countries_events\u0027] \u003d \"NA\"\ndf[\u0027data_source\u0027] \u003d \"NA\"\ndf[\u0027relationship\u0027] \u003d \"NA\"\n\n\ndf[\u0027status\u0027] \u003d \"pending\"\ndf[\u0027date\u0027] \u003d df.index\n\n\nstarting_date \u003d 20210000000000\nending_date   \u003d 20220101000000\ndf \u003d df[(df[\"date\"] \u003e\u003d starting_date)]# \u0026 (df[\"date\"] \u003c\u003d ending_date)]\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python.ipython\ndef fetch_prod_dates(session):\n    dates \u003d []\n    try:\n        rows \u003d session.execute(\u0027SELECT date FROM importstatus\u0027)\n        for idx, user_row in enumerate(rows):\n            dates.append(user_row.date)\n            \n    except Exception as e:\n        print(e)\n        print(statement)\n    return pd.DataFrame(dates,columns\u003d[\u0027date\u0027])\n\ndef fetch_gdelt_dates():\n    response \u003d requests.get(\u0027http://data.gdeltproject.org/gdeltv2/masterfilelist.txt\u0027, allow_redirects\u003dTrue)\n    text \u003d response.content.decode()\n    df \u003d pd.DataFrame(re.split(\" |\\n\", text), columns\u003d[\"champs\"])\n    df[\"zip\"] \u003d df[\"champs\"].apply(lambda x: 1 if \".zip\" in x else \"\")\n    df \u003d df[df[\"zip\"] \u003d\u003d 1]\n    df[\"date\"] \u003d df[\"champs\"].apply(lambda x: re.findall(\"2\\d+\", x)[0])\n    df[\"date\"] \u003d df[\"date\"].astype(int)\n    \n    df \u003d df.groupby(\u0027date\u0027).sum()\n    df \u003d df[df[\"zip\"]\u003d\u003d3]\n    df \u003d df.drop([\u0027champs\u0027,\u0027zip\u0027],axis\u003d1)\n    df \u003d df.reset_index(drop\u003dFalse)\n    starting_date \u003d 20210000000000\n    df \u003d df[(df[\"date\"] \u003e\u003d starting_date)]\n    \n    return df\n    \ndef SQL_INSERT_STATEMENT_FROM_DATAFRAME(SOURCE, TARGET):\n        sql_texts \u003d []\n        for index, row in SOURCE.iterrows():\n            stat \u003d \u0027INSERT INTO \u0027+TARGET+\u0027 (\u0027+ str(\u0027, \u0027.join(SOURCE.columns))+ \u0027) VALUES \u0027+ str(tuple(row.values)).replace(\u0027\\\"{\\\u0027\u0027,\"{\u0027\")\n            stat \u003d stat.replace(\"\\\u0027}\\\"\",\"\\\u0027}\")\n            sql_texts.append(stat)\n        return sql_texts\n\ndef get_new_dates(session,FirstTime):\n    df_diff \u003d pd.concat([fetch_prod_dates(session),fetch_gdelt_dates()])[\u0027date\u0027].drop_duplicates(keep\u003dFirstTime)\n    new_dates \u003d pd.DataFrame(df_diff,columns\u003d[\u0027date\u0027])\n    new_dates[\u0027nb_articles_events\u0027] \u003d \"NA\"\n    new_dates[\u0027countries_events\u0027] \u003d \"NA\"\n    new_dates[\u0027data_source\u0027] \u003d \"NA\"\n    new_dates[\u0027relationship\u0027] \u003d \"NA\"\n    \n    new_dates[\u0027status\u0027] \u003d \"td\"\n    return new_dates\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python.ipython\n\nfirstTime \u003d False # \u0027first\u0027 \nnew_dates \u003d get_new_dates(session,firstTime)\nnew_dates\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python.ipython\n\nnew_dates \u003d get_new_dates(session,False)\nnew_dates\n\nSQL_test \u003d SQL_INSERT_STATEMENT_FROM_DATAFRAME(new_dates.fillna(\u0027null\u0027), \u0027importstatus\u0027)\n\nfor statement in SQL_test:\n    try:\n        print(statement+\u0027 IF NOT EXISTS\u0027)\n        #session.execute(statement)\n    except Exception as e:\n        print(e)\n        continue\n    "
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python.ipython\nnew_dates \u003d pd.DataFrame(df_diff,columns\u003d[\u0027date\u0027])\nnew_dates[\u0027nb_articles_events\u0027] \u003d \"NA\"\nnew_dates[\u0027countries_events\u0027] \u003d \"NA\"\nnew_dates[\u0027data_source\u0027] \u003d \"NA\"\nnew_dates[\u0027relationship\u0027] \u003d \"NA\"\nnew_dates[\u0027status\u0027] \u003d \"td\"\nnew_dates"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python.ipython\n\ndef fetch_and_mark_data(session, amount : int ):\n    dates \u003d []\n    try:\n        rows \u003d session.execute(\u0027SELECT date FROM importstatus WHERE status \u003d \\\u0027TD\\\u0027 ALLOW FILTERING\u0027)\n        for idx, user_row in enumerate(rows):\n            dates.append(user_row.date)\n            statement \u003d f\u0027UPDATE importstatus SET status \u003d \\\u0027doing\\\u0027   WHERE date \u003d {user_row.date} IF EXISTS\u0027\n            session.execute(statement )\n            if idx  \u003eamount-2:\n                break\n            \n    except Exception as e:\n        print(e)\n        print(statement)\n    \n    return dates\n    \nto_import \u003d fetch_and_mark_data(session, 5 )  "
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python.ipython\n\ndef make_df(to_import):\n    df \u003d pd.DataFrame(to_import,columns \u003d[\u0027date\u0027])\n    df[\u0027events\u0027] \u003d df[\u0027date\u0027].apply(lambda x:\u0027http://data.gdeltproject.org/gdeltv2/\u0027+str(x)+\u0027.export.CSV.zip\u0027 )\n    df[\u0027mentions\u0027] \u003d df[\u0027date\u0027].apply(lambda x:\u0027http://data.gdeltproject.org/gdeltv2/\u0027+str(x)+\u0027.mentions.CSV.zip\u0027 )\n    df[\u0027gkg\u0027] \u003d df[\u0027date\u0027].apply(lambda x:\u0027http://data.gdeltproject.org/gdeltv2/\u0027+str(x)+\u0027.gkg.csv.zip\u0027 )\n    df.set_index(\u0027date\u0027)\n    return df\n    \nto_import \u003d make_df(to_import).head()"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python.ipython\ndef update_status(lower_lim , upper_lim):\n    try:\n        rows \u003d session.execute(\u0027SELECT date FROM importstatus WHERE status \u003d \\\u0027pending\\\u0027 ALLOW FILTERING\u0027)\n        for idx, user_row in enumerate(rows):\n            if lower_lim \u003c user_row.date and user_row.date \u003c upper_lim:\n                statement \u003d f\u0027UPDATE importstatus SET status \u003d \\\u0027td\\\u0027   WHERE date \u003d {user_row.date} IF EXISTS\u0027\n                session.execute(statement )\n    except Exception as e:\n        print(e)\n        print(statement)\n        \n       \nupdate_status(20210600000000 ,20230000000000)        \n    "
    }
  ]
}